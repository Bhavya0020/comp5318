{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Rice Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group number: Set 1, Group 153\n",
    "##### Student 1 SID: 550403876\n",
    "##### Student 2 SID: ...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Ignore future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the rice dataset: rice-final2.csv\n",
    "\n",
    "# data = pd.read_csv('rice-final2.csv')\n",
    "data = pd.read_csv('test/test-before.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset\n",
    "\n",
    "#Map to numeric values in target class\n",
    "y = data['class'].map({'class1': 0, 'class2': 1})  \n",
    "\n",
    "# Convert feature columns to numeric, swapping errors with NaN\n",
    "X = data.drop(columns='class')\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Impute missing values with mean\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(X)\n",
    "X = imp_mean.transform(X)\n",
    "\n",
    "# Normalize\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0621,0.4999,0.5410,0.2079,0.2594,0.0613,0\n",
      "0.8073,0.7474,0.6721,0.2634,0.2038,0.0586,0\n",
      "0.3105,0.6030,0.4187,0.0000,0.0000,0.0900,0\n",
      "0.3105,0.5618,0.6148,0.3604,0.0000,0.0950,0\n",
      "0.1863,0.8144,0.6230,0.4990,0.4539,0.1597,1\n",
      "0.1863,0.6039,0.4754,0.1525,0.1000,0.0655,1\n",
      "0.6832,0.7114,0.6230,0.0000,0.0000,0.0877,1\n",
      "0.5589,0.5258,0.6230,0.5129,0.0000,0.0869,0\n",
      "0.1242,0.4639,0.5574,0.5822,0.0000,0.1009,1\n",
      "0.2484,0.5722,0.5902,0.6515,0.3835,0.0979,0\n"
     ]
    }
   ],
   "source": [
    "# Print first ten rows of pre-processed dataset to 4 decimal places as per assignment spec\n",
    "# A function is provided to assist\n",
    "\n",
    "def print_data(X, y, n_rows=10):\n",
    "    \"\"\"Takes a numpy data array and target and prints the first ten rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_examples, n_features)\n",
    "        y: numpy array of shape (n_examples)\n",
    "        n_rows: numpy of rows to print\n",
    "    \"\"\"\n",
    "    for example_num in range(n_rows):\n",
    "        for feature in X[example_num]:\n",
    "            print(\"{:.4f}\".format(feature), end=\",\")\n",
    "\n",
    "        if example_num == len(X)-1:\n",
    "            print(y[example_num],end=\"\")\n",
    "        else:\n",
    "            print(y[example_num])\n",
    "\n",
    "print_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the 10 fold stratified cross-validation\n",
    "cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# The stratified folds from cvKFold should be provided to the classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logistic Regression\n",
    "def logregClassifier(X, y):\n",
    "    model = LogisticRegression(random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Naïve Bayes\n",
    "def nbClassifier(X, y):\n",
    "    model = GaussianNB()\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def dtClassifier(X, y):\n",
    "    model = DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging, Ada Boost and Gradient Boosting Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth):\n",
    "    model = BaggingClassifier(DecisionTreeClassifier(max_depth=max_depth, criterion='entropy', random_state=0),\n",
    "                      n_estimators=n_estimators, max_samples=max_samples, random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth):\n",
    "    model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=max_depth, criterion='entropy', random_state=0),\n",
    "                      n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "def gbClassifier(X, y, n_estimators, learning_rate):\n",
    "    model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogR average cross-validation accuracy: 0.6700\n",
      "NB average cross-validation accuracy: 0.6555\n",
      "DT average cross-validation accuracy: 0.7750\n",
      "Bagging average cross-validation accuracy: 0.7514\n",
      "AdaBoost average cross-validation accuracy: 0.7610\n",
      "GB average cross-validation accuracy: 0.7464\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Part 1:\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 50\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 5\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 50\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 5\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 50\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "# Print results for each classifier in part 1 to 4 decimal places here:\n",
    "print(\"LogR average cross-validation accuracy: {:.4f}\".format(logregClassifier(X,y)))\n",
    "print(\"NB average cross-validation accuracy: {:.4f}\".format(nbClassifier(X,y)))\n",
    "print(\"DT average cross-validation accuracy: {:.4f}\".format(dtClassifier(X,y)))\n",
    "print(\"Bagging average cross-validation accuracy: {:.4f}\".format(bagDTClassifier(X,y,bag_n_estimators,bag_max_samples,bag_max_depth)))\n",
    "print(\"AdaBoost average cross-validation accuracy: {:.4f}\".format(adaDTClassifier(X,y,ada_n_estimators,ada_learning_rate,ada_bag_max_depth)))\n",
    "print(\"GB average cross-validation accuracy: {:.4f}\".format(gbClassifier(X,y,gb_n_estimators,gb_learning_rate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training and testing dataset 75% training, 25% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = [1, 3, 5, 7]\n",
    "p = [1, 2]\n",
    "\n",
    "def bestKNNClassifier(X, y):\n",
    "    param_grid = {'n_neighbors': k, 'p': p}\n",
    "    knn = KNeighborsClassifier()\n",
    "    grid_search = GridSearchCV(knn, param_grid, cv=cvKFold, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_knn = grid_search.best_estimator_\n",
    "    y_pred = best_knn.predict(X_test)\n",
    "    \n",
    "    return grid_search.best_params_['n_neighbors'], grid_search.best_params_['p'], grid_search.best_score_, accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# You should use SVC from sklearn.svm with kernel set to 'rbf'\n",
    "C = [0.01, 0.1, 1, 5] \n",
    "gamma = [0.01, 0.1, 1, 10]\n",
    "\n",
    "def bestSVMClassifier(X, y):\n",
    "    param_grid = {'C': C, 'gamma': gamma}\n",
    "    svm = SVC(kernel='rbf', random_state=0)\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=cvKFold, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)    \n",
    "    best_svm = grid_search.best_estimator_\n",
    "    y_pred = best_svm.predict(X_test)\n",
    "    \n",
    "    return grid_search.best_params_['C'], grid_search.best_params_['gamma'], grid_search.best_score_, accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‘sqrt’.\n",
    "n_estimators = [10, 30, 60, 100]\n",
    "max_leaf_nodes = [6, 12]\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    param_grid = {'n_estimators': n_estimators, 'max_leaf_nodes': max_leaf_nodes}    \n",
    "    rf = RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=0)        \n",
    "    grid_search = GridSearchCV(rf, param_grid, cv=cvKFold, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_svm = grid_search.best_estimator_\n",
    "    y_pred = best_svm.predict(X_test)\n",
    "    \n",
    "    return grid_search.best_params_['n_estimators'], grid_search.best_params_['max_leaf_nodes'], \\\n",
    "        grid_search.best_score_, accuracy_score(y_test, y_pred), \\\n",
    "        f1_score(y_test, y_pred, average='macro'), \\\n",
    "        f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best k: 1.0000\n",
      "KNN best p: 1.0000\n",
      "KNN cross-validation accuracy: 0.7329\n",
      "KNN test set accuracy: 0.6415\n",
      "\n",
      "SVM best C: 5.0000\n",
      "SVM best gamma: 10.0000\n",
      "SVM cross-validation accuracy: 0.6858\n",
      "SVM test set accuracy: 0.5849\n",
      "\n",
      "RF best n_estimators: 60.0000\n",
      "RF best max_leaf_nodes: 12.0000\n",
      "RF cross-validation accuracy: 0.7883\n",
      "RF test set accuracy: 0.6792\n",
      "RF test set macro average F1: 0.6674\n",
      "RF test set weighted average F1: 0.6781\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid Search with 10-fold stratified cross-validation (GridSearchCV in sklearn). \n",
    "# The stratified folds from cvKFold should be provided to GridSearchV\n",
    "\n",
    "# This should include using train_test_split from sklearn.model_selection with stratification and random_state=0\n",
    "# Print results for each classifier here. All results should be printed to 4 decimal places except for\n",
    "# \"k\", \"p\", n_estimators\" and \"max_leaf_nodes\" which should be printed as integers.\n",
    "knn = bestKNNClassifier(X,y)\n",
    "print(\"KNN best k: {:.4f}\".format(knn[0]))\n",
    "print(\"KNN best p: {:.4f}\".format(knn[1]))\n",
    "print(\"KNN cross-validation accuracy: {:.4f}\".format(knn[2]))\n",
    "print(\"KNN test set accuracy: {:.4f}\".format(knn[3]))\n",
    "\n",
    "print()\n",
    "svm = bestSVMClassifier(X,y)\n",
    "print(\"SVM best C: {:.4f}\".format(svm[0]))\n",
    "print(\"SVM best gamma: {:.4f}\".format(svm[1]))\n",
    "print(\"SVM cross-validation accuracy: {:.4f}\".format(svm[2]))\n",
    "print(\"SVM test set accuracy: {:.4f}\".format(svm[3]))\n",
    "\n",
    "print()\n",
    "rf = bestRFClassifier(X,y)\n",
    "print(\"RF best n_estimators: {:.4f}\".format(rf[0]))\n",
    "print(\"RF best max_leaf_nodes: {:.4f}\".format(rf[1]))\n",
    "print(\"RF cross-validation accuracy: {:.4f}\".format(rf[2]))\n",
    "print(\"RF test set accuracy: {:.4f}\".format(rf[3]))\n",
    "print(\"RF test set macro average F1: {:.4f}\".format(rf[4]))\n",
    "print(\"RF test set weighted average F1: {:.4f}\".format(rf[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write one paragraph describing the most important thing that you have learned throughout this assignment.\n",
    "##### Student 1: \n",
    "I focused on data preprocessing and Part 1: \"Cross-validation without parameter tuning\" in this assignment. I expected the ensemble methods (Bagging, AdaBoost and Gradient boost) to perform sifnificantly better than the individual classifiers (logistic regression, naive bayes, and decision trees). While they did perform better than two of them, they were all beaten by decision trees in terms of accuracy. There can be several reasons for this: \n",
    "- The dataset is not complex and/or noisy enough for ensemble methods to shine.\n",
    "- The dataset has very clear boundaries between classes, making it a good fit for decision trees.\n",
    "- Lack of parameer tuning - the ensemble methods should have been tuned to perform better.\n",
    "\n",
    "Either way, the key takeaway for me was that there is no one-size-fits-all model. While one can make educated guesses about which models will perform better than others, you should always test them out before making a decision.\n",
    "##### Student 2: \n",
    "I focused on Part-2: \"Cross-validation with parameter tuning\" and overall review of the codes in this assignment. Throughout the experimentation I saw the trend that increase in cross validation (CV) scores improves testing accuracy which means that CV set is correlated with testing set, which means models which have the highest CV will perform the best on unseen data, so in my part RandomForest was the model with highest CV score of 0.7883 which perfectly makes sense as RandomForest is more robust to noise and overfitting because of it's tree base ensemble. Parameter Tuned RandomForest outperforms Bagging, AdaBoost and Gradient boost because hyper-parameter tuning provides a performance boost to the models thus random forest has optimal parameters compared to other models, had the been tuned their performance might have been better but would have required more time and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
